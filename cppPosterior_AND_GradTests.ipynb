{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "\n",
    "# Memory Tracker\n",
    "\n",
    "def get_current_process_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss  # Returning Resident Set Size (physical memory usage)\n",
    "\n",
    "def run_function_and_get_memory(target_function, num_runs=5, sleep_time=1):\n",
    "    memory_usages = []\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        # Run the target function\n",
    "        target_function()\n",
    "        \n",
    "        # Wait for a bit to ensure the process memory usage is stable\n",
    "        time.sleep(sleep_time)\n",
    "        \n",
    "        # Get the memory usage of the current process\n",
    "        memory_usage = get_current_process_memory_usage()\n",
    "        \n",
    "        # Add to the list if memory usage is found\n",
    "        if memory_usage:\n",
    "            memory_usages.append(memory_usage)\n",
    "        \n",
    "    if memory_usages:\n",
    "        memory_usage = np.array(memory_usages)\n",
    "        m = memory_usage.mean()\n",
    "        std = memory_usage.std()\n",
    "        return m, std\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_minus_log_posterior_and_gradient (__main__.TestLogPosteriorAndGradient.test_minus_log_posterior_and_gradient) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.012s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: combined function correctly implemented\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "from customProphet import *\n",
    "\n",
    "class TestLogPosteriorAndGradient(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Load the shared library\n",
    "        self.lib = ctypes.CDLL('./libminus_log_posterior_and_gradient.so')\n",
    "        self.lib.minus_log_posterior_and_gradient.argtypes = [\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            ctypes.c_size_t,\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            ctypes.c_size_t,\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            ctypes.c_size_t,\n",
    "            ctypes.c_double,\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            ctypes.c_size_t,\n",
    "            ctypes.c_double,\n",
    "            ctypes.c_double,\n",
    "            ctypes.c_double,\n",
    "            ctypes.c_double,\n",
    "            ctypes.c_double,\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS')\n",
    "        ]\n",
    "        self.lib.minus_log_posterior_and_gradient.restype = None\n",
    "\n",
    "        # Load data\n",
    "        df = pd.read_csv('peyton_manning.csv')\n",
    "\n",
    "        # Instantiate & initialize a model\n",
    "        self.model = CustomProphet()\n",
    "        self.model.y = df['y'].values\n",
    "        if df['ds'].dtype != 'datetime64[ns]':\n",
    "            self.model.ds = pd.to_datetime(df['ds'])\n",
    "        else:\n",
    "            self.model.ds = df['ds']\n",
    "\n",
    "        self.model.t_scaled = np.array((self.model.ds - self.model.ds.min()) / (self.model.ds.max() - self.model.ds.min()))\n",
    "        self.model.T = df.shape[0]\n",
    "\n",
    "        self.model.scale_period = (self.model.ds.max() - self.model.ds.min()).days\n",
    "        self.model._normalize_y()\n",
    "        self.model._generate_change_points()\n",
    "\n",
    "        self.params = np.ones((47,))\n",
    "    \n",
    "    def tearDown(self):\n",
    "        # This method is called after each test\n",
    "        print('Test passed: combined function correctly implemented')\n",
    "\n",
    "    def test_minus_log_posterior_and_gradient(self):\n",
    "        # Convert data to ctypes\n",
    "        params_ctypes = np.ascontiguousarray(self.params, dtype=np.float64)\n",
    "        t_scaled_ctypes = np.ascontiguousarray(self.model.t_scaled, dtype=np.float64)\n",
    "        change_points_ctypes = np.ascontiguousarray(self.model.change_points, dtype=np.float64)\n",
    "        normalized_y_ctypes = np.ascontiguousarray(self.model.normalized_y, dtype=np.float64)\n",
    "\n",
    "        mlp_out = np.zeros((1,), dtype=np.float64)\n",
    "        grad_out = np.zeros((47,), dtype=np.float64)\n",
    "\n",
    "        # Call the Python method\n",
    "        mpl_python, grad_python = self.model._minus_log_posteriorAndGradient(self.params)\n",
    "\n",
    "        # Call the C++ function\n",
    "        self.lib.minus_log_posterior_and_gradient(\n",
    "            params_ctypes, len(params_ctypes),\n",
    "            t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "            change_points_ctypes, len(change_points_ctypes),\n",
    "            self.model.scale_period,\n",
    "            normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "            self.model.sigma_obs,\n",
    "            self.model.sigma_k,\n",
    "            self.model.sigma_m,\n",
    "            self.model.sigma,\n",
    "            self.model.tau,\n",
    "            mlp_out,\n",
    "            grad_out\n",
    "        )\n",
    "\n",
    "        # Check if outputs match\n",
    "        np.testing.assert_almost_equal(mlp_out[0], mpl_python, decimal=5, err_msg=\"Log-posterior mismatch\")\n",
    "        np.testing.assert_almost_equal(grad_out, grad_python, decimal=5, err_msg=\"Gradient mismatch\")\n",
    "# Run in a jupyter notebook\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestLogPosteriorAndGradient)\n",
    "\n",
    "# Run the test suite\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python loss: 3594756527.386767\n",
      "Python gradient: [4.53910463e+08 6.27722325e+08 4.34082096e+08 4.14319290e+08\n",
      " 3.94554629e+08 3.74729689e+08 3.55168271e+08 3.35701626e+08\n",
      " 3.16288040e+08 2.96918974e+08 2.77885560e+08 2.59034011e+08\n",
      " 2.40338494e+08 2.21843054e+08 2.03823971e+08 1.86141436e+08\n",
      " 1.68782897e+08 1.51841098e+08 1.35530190e+08 1.19756852e+08\n",
      " 1.04529979e+08 9.00287665e+07 7.64151900e+07 6.36126049e+07\n",
      " 5.16235480e+07 4.07242028e+07 3.09840090e+07 7.98137483e+07\n",
      " 7.77656086e+07 6.96585489e+07 6.23789275e+07 5.45990584e+07\n",
      " 5.24717230e+07 5.08324128e+07 5.27016116e+07 5.65869075e+07\n",
      " 5.64630459e+07 3.02496570e+07 5.08828269e+07 6.02259051e+07\n",
      " 6.40012391e+07 6.36707278e+07 5.99211057e+07 5.63612029e+07\n",
      " 5.47744602e+07 5.35111771e+07 5.50934614e+07]\n",
      "C++ loss: 3594756527.386767\n",
      "C++ gradient: [4.53910463e+08 6.27722325e+08 4.34082096e+08 4.14319290e+08\n",
      " 3.94554629e+08 3.74729689e+08 3.55168271e+08 3.35701626e+08\n",
      " 3.16288040e+08 2.96918974e+08 2.77885560e+08 2.59034011e+08\n",
      " 2.40338494e+08 2.21843054e+08 2.03823971e+08 1.86141436e+08\n",
      " 1.68782897e+08 1.51841098e+08 1.35530190e+08 1.19756852e+08\n",
      " 1.04529979e+08 9.00287665e+07 7.64151900e+07 6.36126049e+07\n",
      " 5.16235480e+07 4.07242028e+07 3.09840090e+07 7.98137483e+07\n",
      " 7.77656086e+07 6.96585489e+07 6.23789275e+07 5.45990584e+07\n",
      " 5.24717230e+07 5.08324128e+07 5.27016116e+07 5.65869075e+07\n",
      " 5.64630459e+07 3.02496570e+07 5.08828269e+07 6.02259051e+07\n",
      " 6.40012391e+07 6.36707278e+07 5.99211057e+07 5.63612029e+07\n",
      " 5.47744602e+07 5.35111771e+07 5.50934614e+07]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "from customProphet import *\n",
    "\n",
    "# Load the shared library\n",
    "lib = ctypes.CDLL('./libminus_log_posterior_and_gradient.so')\n",
    "\n",
    "# Define argument and return types for the C++ function\n",
    "lib.minus_log_posterior_and_gradient.argtypes = [\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    ctypes.c_double,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS')\n",
    "]\n",
    "lib.minus_log_posterior_and_gradient.restype = None\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('peyton_manning.csv')\n",
    "\n",
    "# Instantiate & initialize a model\n",
    "model = CustomProphet()\n",
    "model.y = df['y'].values\n",
    "if df['ds'].dtype != 'datetime64[ns]':\n",
    "    model.ds = pd.to_datetime(df['ds'])\n",
    "else:\n",
    "    model.ds = df['ds']\n",
    "\n",
    "model.t_scaled = np.array((model.ds - model.ds.min()) / (model.ds.max() - model.ds.min()))\n",
    "model.T = df.shape[0]\n",
    "\n",
    "model.scale_period = (model.ds.max() - model.ds.min()).days\n",
    "model._normalize_y()\n",
    "model._generate_change_points()\n",
    "\n",
    "params = np.ones((47,))\n",
    "\n",
    "# Convert data to ctypes\n",
    "params_ctypes = np.ascontiguousarray(params, dtype=np.float64)\n",
    "t_scaled_ctypes = np.ascontiguousarray(model.t_scaled, dtype=np.float64)\n",
    "change_points_ctypes = np.ascontiguousarray(model.change_points, dtype=np.float64)\n",
    "normalized_y_ctypes = np.ascontiguousarray(model.normalized_y, dtype=np.float64)\n",
    "\n",
    "# Call the Python method\n",
    "python_loss, python_grad = model._minus_log_posteriorAndGradient(params)\n",
    "\n",
    "# Prepare arrays to hold C++ results\n",
    "cpp_loss = np.zeros((1,), dtype=np.float64)\n",
    "cpp_grad = np.zeros((47,), dtype=np.float64)\n",
    "\n",
    "# Call the C++ function\n",
    "lib.minus_log_posterior_and_gradient(\n",
    "    params_ctypes, len(params_ctypes),\n",
    "    t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "    change_points_ctypes, len(change_points_ctypes),\n",
    "    model.scale_period,\n",
    "    normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "    model.sigma_obs,\n",
    "    model.sigma_k,\n",
    "    model.sigma_m,\n",
    "    model.sigma,\n",
    "    model.tau,\n",
    "    cpp_loss,\n",
    "    cpp_grad\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Python loss:\", python_loss)\n",
    "print(\"Python gradient:\", python_grad)\n",
    "print(\"C++ loss:\", cpp_loss[0])\n",
    "print(\"C++ gradient:\", cpp_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434 µs ± 41.5 µs per loop (mean ± std. dev. of 100 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 100 -n 100\n",
    "\n",
    "lib.minus_log_posterior_and_gradient(\n",
    "    params_ctypes, len(params_ctypes),\n",
    "    t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "    change_points_ctypes, len(change_points_ctypes),\n",
    "    model.scale_period,\n",
    "    normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "    model.sigma_obs,\n",
    "    model.sigma_k,\n",
    "    model.sigma_m,\n",
    "    model.sigma,\n",
    "    model.tau,\n",
    "    cpp_loss,\n",
    "    cpp_grad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average memory usage over 30 runs: 66.22921875 MB +/- 6.919423611842044 MB\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    lib.minus_log_posterior_and_gradient(\n",
    "    params_ctypes, len(params_ctypes),\n",
    "    t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "    change_points_ctypes, len(change_points_ctypes),\n",
    "    model.scale_period,\n",
    "    normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "    model.sigma_obs,\n",
    "    model.sigma_k,\n",
    "    model.sigma_m,\n",
    "    model.sigma,\n",
    "    model.tau,\n",
    "    cpp_loss,\n",
    "    cpp_grad\n",
    "    )\n",
    "\n",
    "average_memory_usage, std = run_function_and_get_memory(run, num_runs=100, sleep_time=1)\n",
    "if average_memory_usage:\n",
    "    print(f\"Average memory usage over 30 runs: {average_memory_usage / 1024**2} MB +/- {std / 1024**2} MB\")\n",
    "else:\n",
    "    print(\"Failed to measure memory usage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427 µs ± 20.5 µs per loop (mean ± std. dev. of 100 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 100 -n 100\n",
    "\n",
    "lib.minus_log_posterior_and_gradient(\n",
    "    params_ctypes, len(params_ctypes),\n",
    "    t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "    change_points_ctypes, len(change_points_ctypes),\n",
    "    model.scale_period,\n",
    "    normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "    model.sigma_obs,\n",
    "    model.sigma_k,\n",
    "    model.sigma_m,\n",
    "    model.sigma,\n",
    "    model.tau,\n",
    "    cpp_loss,\n",
    "    cpp_grad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.43 ms ± 361 µs per loop (mean ± std. dev. of 100 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 100 -n 100\n",
    "\n",
    "python_loss, python_grad = model._minus_log_posteriorAndGradient(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average memory usage over 30 runs: 59.34109375 MB +/- 7.346520107535927 MB\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    model._minus_log_posteriorAndGradient(params)\n",
    "    \n",
    "average_memory_usage, std = run_function_and_get_memory(run, num_runs=100, sleep_time=1)\n",
    "if average_memory_usage:\n",
    "    print(f\"Average memory usage over 30 runs: {average_memory_usage / 1024**2} MB +/- {std / 1024**2} MB\")\n",
    "else:\n",
    "    print(\"Failed to measure memory usage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_minus_log_posterior_and_gradient (__main__.TestLogPosteriorAndGradient.test_minus_log_posterior_and_gradient) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.062s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: combined function correctly implemented\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "from customProphet import *\n",
    "\n",
    "class TestLogPosteriorAndGradient(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Load the shared library\n",
    "        self.lib = ctypes.CDLL('./libminus_log_posterior_and_gradient_2.so')\n",
    "        self.lib.minus_log_posterior_and_gradient.argtypes = [\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            ctypes.c_size_t,\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            ctypes.c_size_t,\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            ctypes.c_size_t,\n",
    "            ctypes.c_double,\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            ctypes.c_size_t,\n",
    "            ctypes.c_double,\n",
    "            ctypes.c_double,\n",
    "            ctypes.c_double,\n",
    "            ctypes.c_double,\n",
    "            ctypes.c_double,\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS')\n",
    "        ]\n",
    "        self.lib.minus_log_posterior_and_gradient.restype = None\n",
    "\n",
    "        # Load data\n",
    "        df = pd.read_csv('peyton_manning.csv')\n",
    "\n",
    "        # Instantiate & initialize a model\n",
    "        self.model = CustomProphet()\n",
    "        self.model.y = df['y'].values\n",
    "        if df['ds'].dtype != 'datetime64[ns]':\n",
    "            self.model.ds = pd.to_datetime(df['ds'])\n",
    "        else:\n",
    "            self.model.ds = df['ds']\n",
    "\n",
    "        self.model.t_scaled = np.array((self.model.ds - self.model.ds.min()) / (self.model.ds.max() - self.model.ds.min()))\n",
    "        self.model.T = df.shape[0]\n",
    "\n",
    "        self.model.scale_period = (self.model.ds.max() - self.model.ds.min()).days\n",
    "        self.model._normalize_y()\n",
    "        self.model._generate_change_points()\n",
    "\n",
    "        self.params = np.ones((47,))\n",
    "    \n",
    "    def tearDown(self):\n",
    "        # This method is called after each test\n",
    "        print('Test passed: combined function correctly implemented')\n",
    "\n",
    "    def test_minus_log_posterior_and_gradient(self):\n",
    "        # Convert data to ctypes\n",
    "        params_ctypes = np.ascontiguousarray(self.params, dtype=np.float64)\n",
    "        t_scaled_ctypes = np.ascontiguousarray(self.model.t_scaled, dtype=np.float64)\n",
    "        change_points_ctypes = np.ascontiguousarray(self.model.change_points, dtype=np.float64)\n",
    "        normalized_y_ctypes = np.ascontiguousarray(self.model.normalized_y, dtype=np.float64)\n",
    "\n",
    "        mlp_out = np.zeros((1,), dtype=np.float64)\n",
    "        grad_out = np.zeros((47,), dtype=np.float64)\n",
    "\n",
    "        # Call the Python method\n",
    "        mpl_python, grad_python = self.model._minus_log_posteriorAndGradient(self.params)\n",
    "\n",
    "        # Call the C++ function\n",
    "        self.lib.minus_log_posterior_and_gradient(\n",
    "            params_ctypes, len(params_ctypes),\n",
    "            t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "            change_points_ctypes, len(change_points_ctypes),\n",
    "            self.model.scale_period,\n",
    "            normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "            self.model.sigma_obs,\n",
    "            self.model.sigma_k,\n",
    "            self.model.sigma_m,\n",
    "            self.model.sigma,\n",
    "            self.model.tau,\n",
    "            mlp_out,\n",
    "            grad_out\n",
    "        )\n",
    "\n",
    "        # Check if outputs match\n",
    "        np.testing.assert_almost_equal(mlp_out[0], mpl_python, decimal=5, err_msg=\"Log-posterior mismatch\")\n",
    "        np.testing.assert_almost_equal(grad_out, grad_python, decimal=5, err_msg=\"Gradient mismatch\")\n",
    "# Run in a jupyter notebook\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestLogPosteriorAndGradient)\n",
    "\n",
    "# Run the test suite\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python loss: 19106.191245889222\n",
      "Python gradient: [2349.42929854 3249.05986776 2266.75980306 2164.46975155 2062.17009766\n",
      " 1959.55844878 1858.3107567  1757.55360413 1657.07107316 1556.81897732\n",
      " 1458.30417478 1360.73068588 1263.96480128 1168.23448928 1074.96975169\n",
      "  983.44694692  893.60111154  805.91227332  721.48885286  639.84783933\n",
      "  561.03526554  485.978625    415.5162905   349.25155575  287.19755094\n",
      "  230.7837715   180.36962795  413.11694771  402.51600831  360.55478261\n",
      "  322.87628419  282.60856058  271.59771251  263.11282792  272.78759014\n",
      "  292.89744283  292.2563492   156.5788083   263.37376582  311.73248328\n",
      "  331.27318593  329.56249673  310.15487623  291.72922157  283.51642046\n",
      "  276.97780949  285.16753446]\n",
      "C++ loss: 19106.191245889222\n",
      "C++ gradient: [2349.42929854 3249.05986776 2266.75980306 2164.46975155 2062.17009766\n",
      " 1959.55844878 1858.3107567  1757.55360413 1657.07107316 1556.81897732\n",
      " 1458.30417478 1360.73068588 1263.96480128 1168.23448928 1074.96975169\n",
      "  983.44694692  893.60111154  805.91227332  721.48885286  639.84783933\n",
      "  561.03526554  485.978625    415.5162905   349.25155575  287.19755094\n",
      "  230.7837715   180.36962795  413.11694771  402.51600831  360.55478261\n",
      "  322.87628419  282.60856058  271.59771251  263.11282792  272.78759014\n",
      "  292.89744283  292.2563492   156.5788083   263.37376582  311.73248328\n",
      "  331.27318593  329.56249673  310.15487623  291.72922157  283.51642046\n",
      "  276.97780949  285.16753446]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "from customProphet import *\n",
    "\n",
    "# Load the shared library\n",
    "lib = ctypes.CDLL('./libminus_log_posterior_and_gradient_2.so')\n",
    "\n",
    "# Define argument and return types for the C++ function\n",
    "lib.minus_log_posterior_and_gradient.argtypes = [\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    ctypes.c_double,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS')\n",
    "]\n",
    "lib.minus_log_posterior_and_gradient.restype = None\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('peyton_manning.csv')\n",
    "\n",
    "# Instantiate & initialize a model\n",
    "model = CustomProphet()\n",
    "model.y = df['y'].values\n",
    "if df['ds'].dtype != 'datetime64[ns]':\n",
    "    model.ds = pd.to_datetime(df['ds'])\n",
    "else:\n",
    "    model.ds = df['ds']\n",
    "\n",
    "model.t_scaled = np.array((model.ds - model.ds.min()) / (model.ds.max() - model.ds.min()))\n",
    "model.T = df.shape[0]\n",
    "\n",
    "model.scale_period = (model.ds.max() - model.ds.min()).days\n",
    "model._normalize_y()\n",
    "model._generate_change_points()\n",
    "\n",
    "params = np.ones((47,))\n",
    "\n",
    "# Convert data to ctypes\n",
    "params_ctypes = np.ascontiguousarray(params, dtype=np.float64)\n",
    "t_scaled_ctypes = np.ascontiguousarray(model.t_scaled, dtype=np.float64)\n",
    "change_points_ctypes = np.ascontiguousarray(model.change_points, dtype=np.float64)\n",
    "normalized_y_ctypes = np.ascontiguousarray(model.normalized_y, dtype=np.float64)\n",
    "\n",
    "# Call the Python method\n",
    "python_loss, python_grad = model._minus_log_posteriorAndGradient(params)\n",
    "\n",
    "# Prepare arrays to hold C++ results\n",
    "cpp_loss = np.zeros((1,), dtype=np.float64)\n",
    "cpp_grad = np.zeros((47,), dtype=np.float64)\n",
    "\n",
    "# Call the C++ function\n",
    "lib.minus_log_posterior_and_gradient(\n",
    "    params_ctypes, len(params_ctypes),\n",
    "    t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "    change_points_ctypes, len(change_points_ctypes),\n",
    "    model.scale_period,\n",
    "    normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "    model.sigma_obs,\n",
    "    model.sigma_k,\n",
    "    model.sigma_m,\n",
    "    model.sigma,\n",
    "    model.tau,\n",
    "    cpp_loss,\n",
    "    cpp_grad\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Python loss:\", python_loss)\n",
    "print(\"Python gradient:\", python_grad)\n",
    "print(\"C++ loss:\", cpp_loss[0])\n",
    "print(\"C++ gradient:\", cpp_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413 µs ± 21.8 µs per loop (mean ± std. dev. of 100 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 100 -n 100\n",
    "\n",
    "lib.minus_log_posterior_and_gradient(\n",
    "    params_ctypes, len(params_ctypes),\n",
    "    t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "    change_points_ctypes, len(change_points_ctypes),\n",
    "    model.scale_period,\n",
    "    normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "    model.sigma_obs,\n",
    "    model.sigma_k,\n",
    "    model.sigma_m,\n",
    "    model.sigma,\n",
    "    model.tau,\n",
    "    cpp_loss,\n",
    "    cpp_grad\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Memory tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average memory usage over 30 runs: 63.14453125 MB +/- 8.962496560919407 MB\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    lib.minus_log_posterior_and_gradient(\n",
    "    params_ctypes, len(params_ctypes),\n",
    "    t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "    change_points_ctypes, len(change_points_ctypes),\n",
    "    model.scale_period,\n",
    "    normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "    model.sigma_obs,\n",
    "    model.sigma_k,\n",
    "    model.sigma_m,\n",
    "    model.sigma,\n",
    "    model.tau,\n",
    "    cpp_loss,\n",
    "    cpp_grad\n",
    "    )\n",
    "\n",
    "average_memory_usage, std = run_function_and_get_memory(run, num_runs=100, sleep_time=1)\n",
    "if average_memory_usage:\n",
    "    print(f\"Average memory usage over 30 runs: {average_memory_usage / 1024**2} MB +/- {std / 1024**2} MB\")\n",
    "else:\n",
    "    print(\"Failed to measure memory usage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "from customProphet import *\n",
    "\n",
    "# Load the shared library\n",
    "lib = ctypes.CDLL('./libminus_log_posterior_and_gradient.so')\n",
    "\n",
    "# Define argument and return types for the C++ function\n",
    "lib.minus_log_posterior_and_gradient.argtypes = [\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    ctypes.c_double,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS')\n",
    "]\n",
    "lib.minus_log_posterior_and_gradient.restype = None\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('peyton_manning.csv')\n",
    "\n",
    "# Instantiate & initialize a model\n",
    "model = CustomProphet()\n",
    "model.y = df['y'].values\n",
    "if df['ds'].dtype != 'datetime64[ns]':\n",
    "    model.ds = pd.to_datetime(df['ds'])\n",
    "else:\n",
    "    model.ds = df['ds']\n",
    "\n",
    "model.t_scaled = np.array((model.ds - model.ds.min()) / (model.ds.max() - model.ds.min()))\n",
    "model.T = df.shape[0]\n",
    "\n",
    "model.scale_period = (model.ds.max() - model.ds.min()).days\n",
    "model._normalize_y()\n",
    "model._generate_change_points()\n",
    "\n",
    "params = np.ones((47,))\n",
    "\n",
    "# Convert data to ctypes\n",
    "params_ctypes = np.ascontiguousarray(params, dtype=np.float64)\n",
    "t_scaled_ctypes = np.ascontiguousarray(model.t_scaled, dtype=np.float64)\n",
    "change_points_ctypes = np.ascontiguousarray(model.change_points, dtype=np.float64)\n",
    "normalized_y_ctypes = np.ascontiguousarray(model.normalized_y, dtype=np.float64)\n",
    "\n",
    "# Call the Python method\n",
    "python_loss, python_grad = model._minus_log_posteriorAndGradient(params)\n",
    "\n",
    "# Prepare arrays to hold C++ results\n",
    "cpp_loss = np.zeros((1,), dtype=np.float64)\n",
    "cpp_grad = np.zeros((47,), dtype=np.float64)\n",
    "\n",
    "# Call the C++ function\n",
    "lib.minus_log_posterior_and_gradient(\n",
    "    params_ctypes, len(params_ctypes),\n",
    "    t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "    change_points_ctypes, len(change_points_ctypes),\n",
    "    model.scale_period,\n",
    "    normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "    model.sigma_obs,\n",
    "    model.sigma_k,\n",
    "    model.sigma_m,\n",
    "    model.sigma,\n",
    "    model.tau,\n",
    "    cpp_loss,\n",
    "    cpp_grad\n",
    ")\n",
    "\n",
    "def wrapper(params):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
