{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_minus_log_posterior_and_gradient (__main__.TestLogPosteriorAndGradient.test_minus_log_posterior_and_gradient) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.953s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: combined function correctly implemented\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "from customProphet import *\n",
    "\n",
    "class TestLogPosteriorAndGradient(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Load the shared library\n",
    "        self.lib = ctypes.CDLL('./libminus_log_posterior_and_gradient.so')\n",
    "        self.lib.minus_log_posterior_and_gradient.argtypes = [\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            ctypes.c_size_t,\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            ctypes.c_size_t,\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            ctypes.c_size_t,\n",
    "            ctypes.c_double,\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            ctypes.c_size_t,\n",
    "            ctypes.c_double,\n",
    "            ctypes.c_double,\n",
    "            ctypes.c_double,\n",
    "            ctypes.c_double,\n",
    "            ctypes.c_double,\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "            np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS')\n",
    "        ]\n",
    "        self.lib.minus_log_posterior_and_gradient.restype = None\n",
    "\n",
    "        # Load data\n",
    "        df = pd.read_csv('peyton_manning.csv')\n",
    "\n",
    "        # Instantiate & initialize a model\n",
    "        self.model = CustomProphet()\n",
    "        self.model.y = df['y'].values\n",
    "        if df['ds'].dtype != 'datetime64[ns]':\n",
    "            self.model.ds = pd.to_datetime(df['ds'])\n",
    "        else:\n",
    "            self.model.ds = df['ds']\n",
    "\n",
    "        self.model.t_scaled = np.array((self.model.ds - self.model.ds.min()) / (self.model.ds.max() - self.model.ds.min()))\n",
    "        self.model.T = df.shape[0]\n",
    "\n",
    "        self.model.scale_period = (self.model.ds.max() - self.model.ds.min()).days\n",
    "        self.model._normalize_y()\n",
    "        self.model._generate_change_points()\n",
    "\n",
    "        self.params = np.ones((47,))\n",
    "    \n",
    "    def tearDown(self):\n",
    "        # This method is called after each test\n",
    "        print('Test passed: combined function correctly implemented')\n",
    "\n",
    "    def test_minus_log_posterior_and_gradient(self):\n",
    "        # Convert data to ctypes\n",
    "        params_ctypes = np.ascontiguousarray(self.params, dtype=np.float64)\n",
    "        t_scaled_ctypes = np.ascontiguousarray(self.model.t_scaled, dtype=np.float64)\n",
    "        change_points_ctypes = np.ascontiguousarray(self.model.change_points, dtype=np.float64)\n",
    "        normalized_y_ctypes = np.ascontiguousarray(self.model.normalized_y, dtype=np.float64)\n",
    "\n",
    "        mlp_out = np.zeros((1,), dtype=np.float64)\n",
    "        grad_out = np.zeros((47,), dtype=np.float64)\n",
    "\n",
    "        # Call the Python method\n",
    "        mpl_python, grad_python = self.model._minus_log_posteriorAndGradient(self.params)\n",
    "\n",
    "        # Call the C++ function\n",
    "        self.lib.minus_log_posterior_and_gradient(\n",
    "            params_ctypes, len(params_ctypes),\n",
    "            t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "            change_points_ctypes, len(change_points_ctypes),\n",
    "            self.model.scale_period,\n",
    "            normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "            self.model.sigma_obs,\n",
    "            self.model.sigma_k,\n",
    "            self.model.sigma_m,\n",
    "            self.model.sigma,\n",
    "            self.model.tau,\n",
    "            mlp_out,\n",
    "            grad_out\n",
    "        )\n",
    "\n",
    "        # Check if outputs match\n",
    "        np.testing.assert_almost_equal(mlp_out[0], mpl_python, decimal=5, err_msg=\"Log-posterior mismatch\")\n",
    "        np.testing.assert_almost_equal(grad_out, grad_python, decimal=5, err_msg=\"Gradient mismatch\")\n",
    "# Run in a jupyter notebook\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestLogPosteriorAndGradient)\n",
    "\n",
    "# Run the test suite\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python loss: 506.85141876508357\n",
      "Python gradient: [ 0.887452    1.2119592  20.81043235 20.77353514 20.73663447 20.69962126\n",
      " 20.66310004 20.62675576 20.59051055 20.55434845 20.51881302 20.48361713\n",
      " 20.44871255 20.41418152 20.38053984 20.3475265  20.31511807 20.28348768\n",
      " 20.25303518 20.22358631 20.1951577  20.1680839  20.14266732 20.11876486\n",
      " 20.09638126 20.07603215 20.05784719  0.15901247  0.15518859  0.14005269\n",
      "  0.12646162  0.11193658  0.10796484  0.10490425  0.10839404  0.1156479\n",
      "  0.11541665  0.06647619  0.10499837  0.12244192  0.12949048  0.12887341\n",
      "  0.12187286  0.11522651  0.11226406  0.10990551  0.11285963]\n",
      "C++ loss: 506.85141876508357\n",
      "C++ gradient: [ 0.887452    1.2119592  20.81043235 20.77353514 20.73663447 20.69962126\n",
      " 20.66310004 20.62675576 20.59051055 20.55434845 20.51881302 20.48361713\n",
      " 20.44871255 20.41418152 20.38053984 20.3475265  20.31511807 20.28348768\n",
      " 20.25303518 20.22358631 20.1951577  20.1680839  20.14266732 20.11876486\n",
      " 20.09638126 20.07603215 20.05784719  0.15901247  0.15518859  0.14005269\n",
      "  0.12646162  0.11193658  0.10796484  0.10490425  0.10839404  0.1156479\n",
      "  0.11541665  0.06647619  0.10499837  0.12244192  0.12949048  0.12887341\n",
      "  0.12187286  0.11522651  0.11226406  0.10990551  0.11285963]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "from customProphet import *\n",
    "\n",
    "# Load the shared library\n",
    "lib = ctypes.CDLL('./libminus_log_posterior_and_gradient.so')\n",
    "\n",
    "# Define argument and return types for the C++ function\n",
    "lib.minus_log_posterior_and_gradient.argtypes = [\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    ctypes.c_double,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_size_t,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    ctypes.c_double,\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS'),\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, ndim=1, flags='C_CONTIGUOUS')\n",
    "]\n",
    "lib.minus_log_posterior_and_gradient.restype = None\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('peyton_manning.csv')\n",
    "\n",
    "# Instantiate & initialize a model\n",
    "model = CustomProphet()\n",
    "model.y = df['y'].values\n",
    "if df['ds'].dtype != 'datetime64[ns]':\n",
    "    model.ds = pd.to_datetime(df['ds'])\n",
    "else:\n",
    "    model.ds = df['ds']\n",
    "\n",
    "model.t_scaled = np.array((model.ds - model.ds.min()) / (model.ds.max() - model.ds.min()))\n",
    "model.T = df.shape[0]\n",
    "\n",
    "model.scale_period = (model.ds.max() - model.ds.min()).days\n",
    "model._normalize_y()\n",
    "model._generate_change_points()\n",
    "\n",
    "params = np.ones((47,))\n",
    "\n",
    "# Convert data to ctypes\n",
    "params_ctypes = np.ascontiguousarray(params, dtype=np.float64)\n",
    "t_scaled_ctypes = np.ascontiguousarray(model.t_scaled, dtype=np.float64)\n",
    "change_points_ctypes = np.ascontiguousarray(model.change_points, dtype=np.float64)\n",
    "normalized_y_ctypes = np.ascontiguousarray(model.normalized_y, dtype=np.float64)\n",
    "\n",
    "# Call the Python method\n",
    "python_loss, python_grad = model._minus_log_posteriorAndGradient(params)\n",
    "\n",
    "# Prepare arrays to hold C++ results\n",
    "cpp_loss = np.zeros((1,), dtype=np.float64)\n",
    "cpp_grad = np.zeros((47,), dtype=np.float64)\n",
    "\n",
    "# Call the C++ function\n",
    "lib.minus_log_posterior_and_gradient(\n",
    "    params_ctypes, len(params_ctypes),\n",
    "    t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "    change_points_ctypes, len(change_points_ctypes),\n",
    "    model.scale_period,\n",
    "    normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "    model.sigma_obs,\n",
    "    model.sigma_k,\n",
    "    model.sigma_m,\n",
    "    model.sigma,\n",
    "    model.tau,\n",
    "    cpp_loss,\n",
    "    cpp_grad\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Python loss:\", python_loss)\n",
    "print(\"Python gradient:\", python_grad)\n",
    "print(\"C++ loss:\", cpp_loss[0])\n",
    "print(\"C++ gradient:\", cpp_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433 µs ± 76.8 µs per loop (mean ± std. dev. of 100 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 100 -n 100\n",
    "\n",
    "lib.minus_log_posterior_and_gradient(\n",
    "    params_ctypes, len(params_ctypes),\n",
    "    t_scaled_ctypes, len(t_scaled_ctypes),\n",
    "    change_points_ctypes, len(change_points_ctypes),\n",
    "    model.scale_period,\n",
    "    normalized_y_ctypes, len(normalized_y_ctypes),\n",
    "    model.sigma_obs,\n",
    "    model.sigma_k,\n",
    "    model.sigma_m,\n",
    "    model.sigma,\n",
    "    model.tau,\n",
    "    cpp_loss,\n",
    "    cpp_grad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.43 ms ± 361 µs per loop (mean ± std. dev. of 100 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 100 -n 100\n",
    "\n",
    "python_loss, python_grad = model._minus_log_posteriorAndGradient(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
